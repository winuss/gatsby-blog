{"componentChunkName":"component---src-templates-post-tsx","path":"/nb-text-classification-word2vec","result":{"data":{"post":{"headings":[{"depth":3},{"depth":3},{"depth":3},{"depth":3},{"depth":3},{"depth":2}],"frontmatter":{"title":"NLP - 텍스트 분류 (Word2Vec) - #3","path":"/nb-text-classification-word2vec","tags":["ML-DL","ML","NLP"],"excerpt":"이번에는 word2vec을 활용하여 모델을 구현 해보자.","created":"2019-07-19T00:00:00.000Z","createdPretty":"19 July, 2019","updated":"2019-07-02T00:00:00.000Z","updatedPretty":"02 July, 2019","featuredImage":null},"html":"<p>이번에는 word2vec을 활용하여 모델을 구현 해보자.</p>\n<h3 id=\"word2vec을-활용한-모델-구현\" style=\"position:relative;\"><a href=\"#word2vec%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%ED%98%84\" aria-label=\"word2vec을 활용한 모델 구현 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>word2vec을 활용한 모델 구현</h3>\n<p>word2vec을 활용해 모델을 만들기 위해서는 먼저 각 단어에 대해 word2vec으로 백터화해야 한다.</p>\n<p>word2vec의 경우 단어로 표현된 리스트를 입력값으로 넣어야 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">DATA_IN_PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./data_in/'</span>\nTRAIN_CLEAN_DATA <span class=\"token operator\">=</span> <span class=\"token string\">'train_clean.csv'</span>\n\ntrain_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>DATA_IN_PATH <span class=\"token operator\">+</span> TRAIN_CLEAN_DATA<span class=\"token punctuation\">)</span>\n\nreviews <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nsentiments <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nsentences <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n<span class=\"token keyword\">for</span> review <span class=\"token keyword\">in</span> reviews<span class=\"token punctuation\">:</span>\n    sentences<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>review<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">#review문장내 단어들을 배열로 저장(뜨어쓰기 기준)</span></code></pre></div>\n<h3 id=\"word2vec-백터화\" style=\"position:relative;\"><a href=\"#word2vec-%EB%B0%B1%ED%84%B0%ED%99%94\" aria-label=\"word2vec 백터화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>word2vec 백터화</h3>\n<p>이제 word2vec 모델 학습을 진행하기 앞서 word2vec 모델의 하이퍼 파라미터를 설정해야 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 학습 시 필요한 하이퍼 파라미터</span>\nnum_features <span class=\"token operator\">=</span> <span class=\"token number\">300</span>    <span class=\"token comment\"># 워드 백터 특정값 수</span>\nmin_word_count <span class=\"token operator\">=</span> <span class=\"token number\">40</span>   <span class=\"token comment\"># 단어에 대한 최소 빈도 수</span>\nnum_workers <span class=\"token operator\">=</span> <span class=\"token number\">4</span>       <span class=\"token comment\"># 프로세스 개수</span>\ncontext <span class=\"token operator\">=</span> <span class=\"token number\">10</span>         <span class=\"token comment\"># 컨텍스트 윈도우 크기</span>\ndownsampling <span class=\"token operator\">=</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span>   <span class=\"token comment\"># 다운 샘플링 비율</span></code></pre></div>\n<ul>\n<li>num_fratures : 각 단어에 대한 임베딩된 벡터의 차원을 정한다.</li>\n<li>min<em>word</em>count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</li>\n<li>num_workers : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</li>\n<li>context : word2vec을 수행하기 위한 컨텍스트 윈도우 크기를 지정한다.</li>\n<li>downsampling : word2vec 학습을 수행할 때 더 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다. (보통 0.001이 좋은 성능을 낸다고 한다)</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> logging\nlogging<span class=\"token punctuation\">.</span>basicConfig<span class=\"token punctuation\">(</span><span class=\"token builtin\">format</span><span class=\"token operator\">=</span><span class=\"token string\">'%(asctime)s : %(levelname)s : %(message)s'</span><span class=\"token punctuation\">,</span>\\\n   level<span class=\"token operator\">=</span>logging<span class=\"token punctuation\">.</span>INFO<span class=\"token punctuation\">)</span></code></pre></div>\n<p>로깅을 할 떄 format을 위와 같이 지정하고, 로그 수준을 INFO로 하면 word2vec의 학습과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 볼 수 있다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> gensim<span class=\"token punctuation\">.</span>models <span class=\"token keyword\">import</span> word2vec\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Training model...\"</span><span class=\"token punctuation\">)</span>\n\nmodel <span class=\"token operator\">=</span> word2vec<span class=\"token punctuation\">.</span>Word2Vec<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">,</span>\n                        workers<span class=\"token operator\">=</span>num_workers<span class=\"token punctuation\">,</span>\n                        size<span class=\"token operator\">=</span>num_features<span class=\"token punctuation\">,</span>\n                        min_count<span class=\"token operator\">=</span>min_word_count<span class=\"token punctuation\">,</span>\n                        window<span class=\"token operator\">=</span>context<span class=\"token punctuation\">,</span>\n                        sample<span class=\"token operator\">=</span>downsampling<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">2019-07-12 21:56:43,872 : INFO : collecting all words and their counts\n2019-07-12 21:56:43,872 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n\n\nTraining model...\n\n\n2019-07-12 21:56:44,092 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n2019-07-12 21:56:44,305 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n2019-07-12 21:56:44,406 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n2019-07-12 21:56:44,406 : INFO : Loading a fresh vocabulary\n2019-07-12 21:56:44,443 : INFO : min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n2019-07-12 21:56:44,443 : INFO : min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n2019-07-12 21:56:44,454 : INFO : deleting the raw counts dictionary of 74065 items\n2019-07-12 21:56:44,469 : INFO : sample=0.001 downsamples 30 most-common words\n2019-07-12 21:56:44,469 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n2019-07-12 21:56:44,481 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n2019-07-12 21:56:44,481 : INFO : resetting layer weights\n2019-07-12 21:56:44,577 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n2019-07-12 21:56:45,578 : INFO : EPOCH 1 - PROGRESS: at 53.88% examples, 1354613 words/s, in_qsize 7, out_qsize 0\n2019-07-12 21:56:46,589 : INFO : EPOCH 1 - PROGRESS: at 97.12% examples, 1208841 words/s, in_qsize 8, out_qsize 0\n2019-07-12 21:56:46,625 : INFO : worker thread finished; awaiting finish of 3 more threads\n2019-07-12 21:56:46,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-07-12 21:56:46,639 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-07-12 21:56:46,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-07-12 21:56:46,649 : INFO : EPOCH - 1 : training on 2988089 raw words (2494785 effective words) took 2.1s, 1208654 effective words/s\n2019-07-12 21:56:47,643 : INFO : EPOCH 2 - PROGRESS: at 51.68% examples, 1296637 words/s, in_qsize 7, out_qsize 0\n2019-07-12 21:56:48,565 : INFO : worker thread finished; awaiting finish of 3 more threads\n2019-07-12 21:56:48,565 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-07-12 21:56:48,579 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-07-12 21:56:48,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-07-12 21:56:48,587 : INFO : EPOCH - 2 : training on 2988089 raw words (2494872 effective words) took 1.9s, 1287927 effective words/s\n2019-07-12 21:56:49,584 : INFO : EPOCH 3 - PROGRESS: at 52.03% examples, 1307272 words/s, in_qsize 7, out_qsize 0\n2019-07-12 21:56:50,495 : INFO : worker thread finished; awaiting finish of 3 more threads\n2019-07-12 21:56:50,504 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-07-12 21:56:50,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-07-12 21:56:50,513 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-07-12 21:56:50,513 : INFO : EPOCH - 3 : training on 2988089 raw words (2494507 effective words) took 1.9s, 1297134 effective words/s\n2019-07-12 21:56:51,508 : INFO : EPOCH 4 - PROGRESS: at 52.60% examples, 1319590 words/s, in_qsize 7, out_qsize 0\n2019-07-12 21:56:52,421 : INFO : worker thread finished; awaiting finish of 3 more threads\n2019-07-12 21:56:52,438 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-07-12 21:56:52,438 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-07-12 21:56:52,444 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-07-12 21:56:52,444 : INFO : EPOCH - 4 : training on 2988089 raw words (2494292 effective words) took 1.9s, 1293999 effective words/s\n2019-07-12 21:56:53,445 : INFO : EPOCH 5 - PROGRESS: at 54.20% examples, 1360524 words/s, in_qsize 7, out_qsize 0\n2019-07-12 21:56:54,212 : INFO : worker thread finished; awaiting finish of 3 more threads\n2019-07-12 21:56:54,228 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-07-12 21:56:54,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-07-12 21:56:54,241 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-07-12 21:56:54,242 : INFO : EPOCH - 5 : training on 2988089 raw words (2494012 effective words) took 1.8s, 1390220 effective words/s\n2019-07-12 21:56:54,242 : INFO : training on a 14940445 raw words (12472468 effective words) took 9.7s, 1291189 effective words/s</code></pre></div>\n<p>word2vec으로 학습시킨 모델의 경우 모델을 따로 저장해두면 이후 다시 사용할수 있기 때문에 저장해두자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기 용이하다.</span>\n<span class=\"token comment\"># 모델을 저장하면 Word2Vec.load()를 통해 다시 사용할 수 있다.</span>\nmodel_name <span class=\"token operator\">=</span> <span class=\"token string\">\"300features_40minwords_10context\"</span>\nmodel<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model_name<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">2019-07-12 22:00:40,807 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n2019-07-12 22:00:40,807 : INFO : not storing attribute vectors_norm\n2019-07-12 22:00:40,807 : INFO : not storing attribute cum_table\nC:\\Users\\nicey\\.conda\\envs\\nlp\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n  &#39;See the migration notes for details: %s&#39; % _MIGRATION_NOTES_URL\n2019-07-12 22:00:41,229 : INFO : saved 300features_40minwords_10context</code></pre></div>\n<p>에제 만들어진 word2vec 모델을 활용해 선형 회귀 모델을 학습해보자. 우선 학습을 위해 하나의 리뷰를 같은 형태의 입력값으로 만들어야 한다. 지금은 word2vec 모델에서 각 단어가 벡터로 표현돼 있다. 그리고 리뷰마다 단어의 개수가 모두 다르기 때문에 입력값을 하나으 형태로 만들어야 한다. 가장 단순한 방법은 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법이 있다.</p>\n<p>그럼 하나의 리뷰에 대해 전체 단어의 평균값을 계산하는 함수를 구현하자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_features</span><span class=\"token punctuation\">(</span>words<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> num_features<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># 출력 벡터 초기화</span>\n    feature_vector <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>num_features<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>np<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span>\n    \n    num_words <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n    <span class=\"token comment\"># 어휘 사전 준비</span>\n    index2word_set <span class=\"token operator\">=</span> <span class=\"token builtin\">set</span><span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>wv<span class=\"token punctuation\">.</span>index2word<span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">for</span> w <span class=\"token keyword\">in</span> words<span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> w <span class=\"token keyword\">in</span> index2word_set<span class=\"token punctuation\">:</span>\n            num_words <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\n            <span class=\"token comment\"># 사전에 해당하는 단어에 대해 단어 벡터를 더함</span>\n            feature_vector <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>feature_vector<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">[</span>w<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n            \n    <span class=\"token comment\"># 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함</span>\n    feature_vector <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>divide<span class=\"token punctuation\">(</span>feature_vector<span class=\"token punctuation\">,</span> num_words<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> feature_vector</code></pre></div>\n<ul>\n<li>words : 단어의 모음인 하나의 리뷰가 들어간다.</li>\n<li>model : word2vec 모델을 넣는 공이며, 우리가 학습한 word2vec 모델이 들어간다.</li>\n<li>num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원 수를 뜻한다.</li>\n</ul>\n<p>하나의 벡터를 만드는 과정을 빠르게 하기 위해 np.zeros를 사용해 미리 모두 0값을 가지는 벡터를 만든다. 그리고 문장의 단어가 모델 단어사전에 속하는지 보기 위해 model.wv.index2word를 set객체로 생성해서 index2word_set 변수에 할당한다. 다음 반복문을 통해 리뷰를 구성하는 단어에 대해 임베딩된 벡터가 있는 단어 벡터의 합을 구하고 사용한 단어의 전체 개수로 나누어 평균 벡터의 값을 구한다.</p>\n<p>문장에 특징값을 만들 수 있는 함수를 구현했다면 이제 앞에서 정의한 함수를 사용해 전체 리뷰에 대해 각 리뷰의 평균 벡터를 구하는 함수를 정의하자</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">get_dataset</span><span class=\"token punctuation\">(</span>reviews<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> num_features<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    dataset <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">for</span> s <span class=\"token keyword\">in</span> reviews<span class=\"token punctuation\">:</span>\n        dataset<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>get_features<span class=\"token punctuation\">(</span>s<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> num_features<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        \n    reviewFeatureVecs <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span>dataset<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> reviewFeatureVecs</code></pre></div>\n<ul>\n<li>reviews : 학습 데이터인 전체 리뷰 데이터를 입력</li>\n<li>model : word2vec 모델을 입력</li>\n<li>num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원 수</li>\n</ul>\n<p>전체 리뷰에 대한 평균 벡터를 담을 0으로 채워진 numpy 배열을 미리 만든다. 배열은 2차원, 배열의 행에는 각 문장에 대한 길이, 열에는 평균 벡터의 차원수 즉 크기를 입력. 그리고 각 리뷰에 대해 반복문을 돌면서 각 리뷰에 대해 특징 값을 만든다.</p>\n<p>구현한 함수를 사용해 실제 학습에 사용될 입력값을 만들어 보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_data_vecs <span class=\"token operator\">=</span> get_dataset<span class=\"token punctuation\">(</span>sentences<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> num_features<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">C:\\Users\\nicey\\.conda\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n  del sys.path[0]</code></pre></div>\n<h3 id=\"학습과-검증-데이터셋-분리\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5%EA%B3%BC-%EA%B2%80%EC%A6%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EB%B6%84%EB%A6%AC\" aria-label=\"학습과 검증 데이터셋 분리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습과 검증 데이터셋 분리</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\nX <span class=\"token operator\">=</span> test_data_vecs\ny <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>sentiments<span class=\"token punctuation\">)</span>\n\nRANDOM_SEED <span class=\"token operator\">=</span> <span class=\"token number\">42</span>\nTEST_SPLIT <span class=\"token operator\">=</span> <span class=\"token number\">0.2</span>\n\nX_train<span class=\"token punctuation\">,</span> X_test<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_test <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span>TEST_SPLIT<span class=\"token punctuation\">,</span> \n                                                    random_state<span class=\"token operator\">=</span>RANDOM_SEED<span class=\"token punctuation\">)</span></code></pre></div>\n<h3 id=\"모델-선언-및-학습\" style=\"position:relative;\"><a href=\"#%EB%AA%A8%EB%8D%B8-%EC%84%A0%EC%96%B8-%EB%B0%8F-%ED%95%99%EC%8A%B5\" aria-label=\"모델 선언 및 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>모델 선언 및 학습</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LogisticRegression\n\nlgs <span class=\"token operator\">=</span> LogisticRegression<span class=\"token punctuation\">(</span>class_weight<span class=\"token operator\">=</span><span class=\"token string\">'balanced'</span><span class=\"token punctuation\">)</span>\nlgs<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">C:\\Users\\nicey\\.conda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n\n\n\n\n\nLogisticRegression(C=1.0, class_weight=&#39;balanced&#39;, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;,\n                   random_state=None, solver=&#39;warn&#39;, tol=0.0001, verbose=0,\n                   warm_start=False)</code></pre></div>\n<p>class_weight을 <code class=\"language-text\">balanced</code>로 설정했다. 이는 각 라벨에 대해 균형있게 학습하기 위함이다.</p>\n<h3 id=\"검증-데이터셋을-이용한-성능-평가\" style=\"position:relative;\"><a href=\"#%EA%B2%80%EC%A6%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80\" aria-label=\"검증 데이터셋을 이용한 성능 평가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>검증 데이터셋을 이용한 성능 평가</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">predicted <span class=\"token operator\">=</span> lgs<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">from</span> sklearn <span class=\"token keyword\">import</span> metrics\n\nfpr<span class=\"token punctuation\">,</span> tpr<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> metrics<span class=\"token punctuation\">.</span>roc_curve<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>lgs<span class=\"token punctuation\">.</span>predict_proba<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nauc <span class=\"token operator\">=</span> metrics<span class=\"token punctuation\">.</span>auc<span class=\"token punctuation\">(</span>fpr<span class=\"token punctuation\">,</span> tpr<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"------------\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Accuracy: %f\"</span> <span class=\"token operator\">%</span> lgs<span class=\"token punctuation\">.</span>score<span class=\"token punctuation\">(</span>X_test<span class=\"token punctuation\">,</span> y_test<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">#checking the accuracy</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Precision: %f\"</span> <span class=\"token operator\">%</span> metrics<span class=\"token punctuation\">.</span>precision_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> predicted<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Recall: %f\"</span> <span class=\"token operator\">%</span> metrics<span class=\"token punctuation\">.</span>recall_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> predicted<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"F1-Score: %f\"</span> <span class=\"token operator\">%</span> metrics<span class=\"token punctuation\">.</span>f1_score<span class=\"token punctuation\">(</span>y_test<span class=\"token punctuation\">,</span> predicted<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"AUC: %f\"</span> <span class=\"token operator\">%</span> auc<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">------------\nAccuracy: 0.874200\nPrecision: 0.869141\nRecall: 0.883287\nF1-Score: 0.876157\nAUC: 0.940927</code></pre></div>\n<p>학습 결과를 확인해 보면 TF-IDF를 사용해서 학습한 것보다 상대적으로 성능이 조금 떨어지는 것을 볼 수 있다. word2vec이 단어 간의 유사도를 보는 관점에서는 분명히 효과적일 수는 있지만 항상 좋은 성능을 보장하지는 않는다는 점을 알 수 있다.</p>\n<h2 id=\"데이터-제출\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%9C%EC%B6%9C\" aria-label=\"데이터 제출 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터 제출</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">TEST_CLEAN_DATA <span class=\"token operator\">=</span> <span class=\"token string\">'test_clean.csv'</span>\n\ntest_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>DATA_IN_PATH <span class=\"token operator\">+</span> TEST_CLEAN_DATA<span class=\"token punctuation\">)</span>\n\ntest_review <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_data<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token number\">5</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>naturally film main themes mortality nostalgia...</td>\n      <td>\"12311_10\"</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>movie disaster within disaster film full great...</td>\n      <td>\"8348_2\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>movie kids saw tonight child loved one point k...</td>\n      <td>\"5828_4\"</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afraid dark left impression several different ...</td>\n      <td>\"7186_2\"</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>accurate depiction small time mob life filmed ...</td>\n      <td>\"12128_7\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_sentences <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> review <span class=\"token keyword\">in</span> test_review<span class=\"token punctuation\">:</span>\n    test_sentences<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>review<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_data_vecs <span class=\"token operator\">=</span> get_dataset<span class=\"token punctuation\">(</span>test_sentences<span class=\"token punctuation\">,</span> model<span class=\"token punctuation\">,</span> num_features<span class=\"token punctuation\">)</span>\ntest_predicted <span class=\"token operator\">=</span> lgs<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>test_data_vecs<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">C:\\Users\\nicey\\.conda\\envs\\nlp\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n  del sys.path[0]</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n\nDATA_OUT_PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./data_out/'</span>\n\n<span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span>DATA_OUT_PATH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span>DATA_OUT_PATH<span class=\"token punctuation\">)</span>\n\nids <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nanswer_dataset <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">:</span> ids<span class=\"token punctuation\">,</span> <span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">:</span> test_predicted<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\nanswer_dataset<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span>DATA_OUT_PATH <span class=\"token operator\">+</span> <span class=\"token string\">'lgs_w2v_answer.csv'</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> quoting<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span></code></pre></div>"},"primaryTag":{"name":"ML-DL","color":"#4bc822"}},"pageContext":{"postId":"38c67cff-274e-5f12-8ddf-70699490c20a","primaryTag":"ML-DL"}}}