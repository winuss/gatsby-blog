{"componentChunkName":"component---src-templates-post-tsx","path":"/nb-text-classification-tf-idf","result":{"data":{"post":{"headings":[{"depth":2},{"depth":2},{"depth":2},{"depth":2}],"frontmatter":{"title":"NLP - 텍스트 분류 (TF-IDF) - #2","path":"/nb-text-classification-tf-idf","tags":["ML-DL","ML","NLP"],"excerpt":"이제 전처리된 데이터를 가지고 TF-IDF를 활용한 모델을 구현할 것이다.","created":"2019-07-02T00:00:00.000Z","createdPretty":"02 July, 2019","updated":"2019-07-02T00:00:00.000Z","updatedPretty":"02 July, 2019","featuredImage":null},"html":"<p>이전 글에서는 데이터를 모델에 적용하기 전에 데이터에 대해 이해하고 정제하는 과정인 데이터 전처리 과정을 진행했다. </p>\n<p>이제 전처리된 데이터를 가지고 TF-IDF를 활용한 모델을 구현할 것이다.</p>\n<blockquote>\n<p><strong>선형 회귀 모델</strong> : 종속변수와 독립변수 간의 상관관계를 모델링하는 방법<br/>\n<strong>로지스틱 회귀 모델</strong> : 선형 모델의 결과값에 로지스틱 함수를 적용하여 0 ~ 1 사이의 값을 갖게 하여 확률로 표현<br/>\n<strong>TF-IDF</strong> : TF(Term Frequency, 단어의 빈도), IDF(역문서 빈도, Inverse Document Frequency) 쉽게 말하자면 문장에서 단어의 빈도수를 계산하되 너무 자주 등장하는 단어는 크게 의미를 두지 않도록 가중치를 낮게 주자는 의미.</p>\n</blockquote>\n<p>TF-IDF를 활용해 문장 벡터를 만들기 위한 TfidfVectorizer를 사용하기 위해서는 입력값이 텍스트로 이루어진 데이터 형태여야 하기 때문에 전처리한 결과 중 numpy배열이 아닌 정제된 텍스트 데이터를 사용해야 한다.</p>\n<blockquote>\n<p>훈련 데이터 : train<em>clean.csv<br/>\n테스트 데이터 : test</em>clean.csv</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> os\n<span class=\"token keyword\">import</span> pandas <span class=\"token keyword\">as</span> pd\n<span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>feature_extraction<span class=\"token punctuation\">.</span>text <span class=\"token keyword\">import</span> TfidfVectorizer</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">DATA_IN_PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./data_in/'</span>\nDATA_OUT_PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./data_out/'</span>\nTRAIN_CLEAN_DATA <span class=\"token operator\">=</span> <span class=\"token string\">'train_clean.csv'</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>DATA_IN_PATH <span class=\"token operator\">+</span> TRAIN_CLEAN_DATA<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">train_data<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>stuff going moment mj started listening music ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>classic war worlds timothy hines entertaining ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>film starts manager nicholas bell giving welco...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>must assumed praised film greatest filmed oper...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>superbly trashy wondrously unpretentious explo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">reviews <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nsentiments <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>train_data<span class=\"token punctuation\">[</span><span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nvectorizer <span class=\"token operator\">=</span> TfidfVectorizer<span class=\"token punctuation\">(</span>\n            min_df<span class=\"token operator\">=</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">,</span>\n            analyzer<span class=\"token operator\">=</span><span class=\"token string\">\"char\"</span><span class=\"token punctuation\">,</span>\n            sublinear_tf<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n            ngram_range<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            max_features<span class=\"token operator\">=</span><span class=\"token number\">5000</span>\n        <span class=\"token punctuation\">)</span>\n\nX <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>fit_transform<span class=\"token punctuation\">(</span>reviews<span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>min_df : 설정한 값보다 특정 토큰의 df값이 더 적게 나오면 벡터화 과정에서 제거</li>\n<li>analyzer : 분석하기 위한 기준 단위(word:단어 하나를 단위로, char:문자 하나를 단위로)</li>\n<li>sublinear_tf : 문서의 단어 빈도 수에 대한 스무딩(smoothing) 여부</li>\n<li>ngram_range : 빈도의 기본 단위를 설정할 n-gram 범위</li>\n<li>max_features : 각 벡터의 최대 길이</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">X</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;25000x5000 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39;\n\twith 17862871 stored elements in Compressed Sparse Row format&gt;</code></pre></div>\n<h2 id=\"학습과-검증-데이터셋-분리\" style=\"position:relative;\"><a href=\"#%ED%95%99%EC%8A%B5%EA%B3%BC-%EA%B2%80%EC%A6%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EB%B6%84%EB%A6%AC\" aria-label=\"학습과 검증 데이터셋 분리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>학습과 검증 데이터셋 분리</h2>\n<p>해당 입력값을 모델에 적용하기전 학습데이터의 일부를 검증 데이터로 따로 분리하자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>model_selection <span class=\"token keyword\">import</span> train_test_split\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n\nRANDOM_SEED <span class=\"token operator\">=</span> <span class=\"token number\">42</span>\nTEST_SPLIT <span class=\"token operator\">=</span> <span class=\"token number\">0.2</span>\n\ny <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>sentiments<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\nX_train<span class=\"token punctuation\">,</span> X_eval<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">,</span> y_eval <span class=\"token operator\">=</span> train_test_split<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">,</span> y<span class=\"token punctuation\">,</span> test_size<span class=\"token operator\">=</span>TEST_SPLIT<span class=\"token punctuation\">,</span> \n                                                    random_state<span class=\"token operator\">=</span>RANDOM_SEED<span class=\"token punctuation\">)</span></code></pre></div>\n<p>입력값인 X와 정답 라벨을 numpy 배열로 만든 y에 대해 적용해서 학습데이터와 검증데이터로 나누었다.</p>\n<h2 id=\"모델-선언-및-학습\" style=\"position:relative;\"><a href=\"#%EB%AA%A8%EB%8D%B8-%EC%84%A0%EC%96%B8-%EB%B0%8F-%ED%95%99%EC%8A%B5\" aria-label=\"모델 선언 및 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>모델 선언 및 학습</h2>\n<p>선형 회귀 모델을 만들기 위해 LogisticRegression을 사용하고, class_weight를 'balanced'로 설정해서 각 라벨에 대해 균형 있게 학습할 수 있게 하자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> sklearn<span class=\"token punctuation\">.</span>linear_model <span class=\"token keyword\">import</span> LogisticRegression\n\nlgs <span class=\"token operator\">=</span> LogisticRegression<span class=\"token punctuation\">(</span>class_weight <span class=\"token operator\">=</span> <span class=\"token string\">'balanced'</span><span class=\"token punctuation\">)</span>\nlgs<span class=\"token punctuation\">.</span>fit<span class=\"token punctuation\">(</span>X_train<span class=\"token punctuation\">,</span> y_train<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">C:\\Users\\nicey\\.conda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to &#39;lbfgs&#39; in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n\n\n\n\n\nLogisticRegression(C=1.0, class_weight=&#39;balanced&#39;, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;,\n                   random_state=None, solver=&#39;warn&#39;, tol=0.0001, verbose=0,\n                   warm_start=False)</code></pre></div>\n<h2 id=\"검증-데이터로-성능-평가\" style=\"position:relative;\"><a href=\"#%EA%B2%80%EC%A6%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A1%9C-%EC%84%B1%EB%8A%A5-%ED%8F%89%EA%B0%80\" aria-label=\"검증 데이터로 성능 평가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>검증 데이터로 성능 평가</h2>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Accuracy : %f\"</span> <span class=\"token operator\">%</span> lgs<span class=\"token punctuation\">.</span>score<span class=\"token punctuation\">(</span>X_eval<span class=\"token punctuation\">,</span> y_eval<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 검증 데이터로 성능 측정</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Accuracy : 0.859600</code></pre></div>\n<p>성능 평가 방법으로 정밀도(precision), 재현율(recall), f1-score, auc 등의 다양한 지표가 있지만 여기서는 정확도(Accuracy)만 측정하였다.</p>\n<p>평가 결과 약 86%의 정확도를 보였다. 성능이 생각보다 나오지 않을 때는 하이퍼파라미터를 수정하거나 다른 기법들을 추가해서 성능을 올려보자. 검증 데이터의 성능이 만족할 만큼 나온다면 평가 데이터를 적용하면 된다.</p>\n<h2 id=\"데이터-제출하기\" style=\"position:relative;\"><a href=\"#%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%9C%EC%B6%9C%ED%95%98%EA%B8%B0\" aria-label=\"데이터 제출하기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>데이터 제출하기</h2>\n<p>생성한 모델을 활용해 평가 데이터 결과를 예측하고 캐글에 제출할 수 있도록 파일로 저장하자.</p>\n<p>우선 전처리한 텍스트 형태의 평가 데이터를 불러오자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">TEST_CLEAN_DATA <span class=\"token operator\">=</span> <span class=\"token string\">'test_clean.csv'</span>\n\ntest_data <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_csv<span class=\"token punctuation\">(</span>DATA_IN_PATH <span class=\"token operator\">+</span> TEST_CLEAN_DATA<span class=\"token punctuation\">,</span> header<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> delimiter<span class=\"token operator\">=</span><span class=\"token string\">\",\"</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_data<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}</code></pre></div>\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>naturally film main themes mortality nostalgia...</td>\n      <td>\"12311_10\"</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>movie disaster within disaster film full great...</td>\n      <td>\"8348_2\"</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>movie kids saw tonight child loved one point k...</td>\n      <td>\"5828_4\"</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>afraid dark left impression several different ...</td>\n      <td>\"7186_2\"</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>accurate depiction small time mob life filmed ...</td>\n      <td>\"12128_7\"</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>해당 데이터를 대상으로 이전에 학습 데이터에 대해 사용했던 객체를 사용해 TF-IDF 값으로 벡터화한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">testDataVecs <span class=\"token operator\">=</span> vectorizer<span class=\"token punctuation\">.</span>transform<span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'review'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>백터화할 때 평가 데이터에 대해서는 fit을 호출하지 않고 그대로 transform만 호툴한다.</p>\n<p>이미 학습 데이터에 맞게 설정했고, 그 설정에 맞게 평가 데이터도 변환을 하면 된다.</p>\n<p>이제 이 값으로 예측한 후 예측값을 하나의 변수로 할당하고 출력해보자.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">test_predicted <span class=\"token operator\">=</span> lgs<span class=\"token punctuation\">.</span>predict<span class=\"token punctuation\">(</span>testDataVecs<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>test_predicted<span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[1 0 1 ... 0 1 0]</code></pre></div>\n<p>결과를 보면 각 데이터에 대해 긍정, 부정 값을 가지고 있다.</p>\n<p>이제 이 값을 캐글에 제출하기 위해 csv 파일로 저장하자. 캐글에 제출하기 위한 데이터 형식은 각 데이터의 고유한 id 값과 결과값으로 구성되어야 한다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">DATA_OUT_PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./data_out/'</span>\n\n<span class=\"token keyword\">if</span> <span class=\"token keyword\">not</span> os<span class=\"token punctuation\">.</span>path<span class=\"token punctuation\">.</span>exists<span class=\"token punctuation\">(</span>DATA_OUT_PATH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    os<span class=\"token punctuation\">.</span>makedirs<span class=\"token punctuation\">(</span>DATA_OUT_PATH<span class=\"token punctuation\">)</span>\n    \nids <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\nanswer_dataset <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">:</span> test_data<span class=\"token punctuation\">[</span><span class=\"token string\">'id'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'sentiment'</span><span class=\"token punctuation\">:</span> test_predicted<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\nanswer_dataset<span class=\"token punctuation\">.</span>to_csv<span class=\"token punctuation\">(</span>DATA_OUT_PATH <span class=\"token operator\">+</span> <span class=\"token string\">'lgs_tfidf_answer.csv'</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> quoting<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span></code></pre></div>"},"primaryTag":{"name":"ML-DL","color":"#4bc822"}},"pageContext":{"postId":"766be125-572b-5d4d-a492-0193a3caa3c1","primaryTag":"ML-DL"}}}